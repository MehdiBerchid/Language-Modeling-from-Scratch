# Language Modeling from Scratch
Implementing the Attention Is All You Need paper from scratch. The goal is to deeply understand vanilla Transformers and optimize them using modern architectures